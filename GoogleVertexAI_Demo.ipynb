{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPq8I0zOBTfdQsz0hRpz8X2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChetanKnowIt/CloudNotes/blob/main/GoogleVertexAI_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\" > Vertex AI Demo </h1>\n",
        "\n",
        "<hr />\n",
        "\n",
        "\n",
        "- this notebook will be used for AI demos\n",
        "- has google cloud connectivitiy using cloud api\n",
        "- 1st demo will be about connecting google sheets and reading a column then searching that pdf and creating a summary from abstract"
      ],
      "metadata": {
        "id": "ULOpR8SgDFOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SETUP"
      ],
      "metadata": {
        "id": "sY3_YnlqDsHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"shapely<2.0.0\"\n",
        "!pip install google-cloud-aiplatform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnG0IYUhDe93",
        "outputId": "d037da9c-967c-4c69-9ea7-4fce25a7a676"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: shapely<2.0.0 in /usr/local/lib/python3.10/dist-packages (1.8.5.post1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.26.1)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.22.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.9.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.10.1)\n",
            "Requirement already satisfied: shapely<2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.1)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.27.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.54.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SETUP & TESTING text-bison@001 model BASED ON PALM2"
      ],
      "metadata": {
        "id": "iZJ30cHaFlYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()\n",
        "\n",
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiproject-390805\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 512,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
      ],
      "metadata": {
        "id": "VBEkQ8daKwy9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response = model.predict(\n",
        "    \"\"\"Provide a summary with about two sentences for the following article: Beyond our own products, we think it\\'s important to make it easy, safe and scalable for others to benefit from these advances by building on top of our best models. Next month, we\\'ll start onboarding individual developers, creators and enterprises so they can try our Generative Language API, initially powered by LaMDA with a range of models to follow. Over time, we intend to create a suite of tools and APIs that will make it easy for others to build more innovative applications with AI. Having the necessary compute power to build reliable and trustworthy AI systems is also crucial to startups, and we are excited to help scale these efforts through our Google Cloud partnerships with Cohere, C3.ai and Anthropic, which was just announced last week. Stay tuned for more developer details soon.\n",
        "Summary: Google is making its AI technology more accessible to developers, creators, and enterprises. Next month, Google will start onboarding developers to try its Generative Language API, which will initially be powered by LaMDA. Over time, Google intends to create a suite of tools and APIs that will make it easy for others to build more innovative applications with AI. Google is also excited to help scale these efforts through its Google Cloud partnerships with Cohere, C3.ai, and Anthropic.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: The benefits of electricPromptData kitchens go beyond climate impact, starting with speed. The first time I ever cooked on induction (electric) equipment, the biggest surprise was just how incredibly fast it is. In fact, induction boils water twice as fast as traditional gas equipment and is far more efficient — because unlike a flame, electric heat has nowhere to escape. At Bay View, our training programs help Google chefs appreciate and adjust to the new pace of induction. The speed truly opens up whole new ways of cooking.\n",
        "Summary: Electric kitchens are faster, more efficient, and better for the environment than gas kitchens. Induction cooking is particularly fast, boiling water twice as fast as traditional gas equipment. This speed opens up whole new ways of cooking. Google chefs are trained to appreciate and adjust to the new pace of induction cooking at Bay View.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: We\\'re also using AI to forecast floods, another extreme weather pattern exacerbated by climate change. We\\'ve already helped communities to predict when floods will hit and how deep the waters will get — in 2021, we sent 115 million flood alert notifications to 23 million people over Google Search and Maps, helping save countless lives. Today, we\\'re sharing that we\\'re now expanding our coverage to more countries in South America (Brazil and Colombia), Sub-Saharan Africa (Burkina Faso, Cameroon, Chad, Democratic Republic of Congo, Ivory Coast, Ghana, Guinea, Malawi, Nigeria, Sierra Leone, Angola, South Sudan, Namibia, Liberia, and South Africa), and South Asia (Sri Lanka). We\\'ve used an AI technique called transfer learning to make it work in areas where there\\'s less data available. We\\'re also announcing the global launch of Google FloodHub, a new platform that displays when and where floods may occur. We\\'ll also be bringing this information to Google Search and Maps in the future to help more people to reach safety in flooding situations.\n",
        "Summary: Google is using AI to forecast floods in South America, Sub-Saharan Africa, South Asia, and other parts of the world. The AI technique of transfer learning is being used to make it work in areas where there\\'s less data available. Google FloodHub, a new platform that displays when and where floods may occur, has also been launched globally. This information will also be brought to Google Search and Maps in the future to help more people reach safety in flooding situations.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: In order to learn skiing, you must first be educated on the proper use of the equipment. This includes learning how to properly fit your boot on your foot, understand the different functions of the ski, and bring gloves, goggles etc. Your instructor starts you with one-footed ski drills. Stepping side-to-side, forward-and-backward, making snow angels while keeping your ski flat to the ground, and gliding with the foot not attached to a ski up for several seconds. Then you can put on both skis and get used to doing them with two skis on at once. Next, before going down the hill, you must first learn how to walk on the flat ground and up small hills through two methods, known as side stepping and herringbone. Now it\\'s time to get skiing! For your first attempted run, you will use the skills you just learned on walking up the hill, to go down a small five foot vertical straight run, in which you will naturally stop on the flat ground. This makes you learn the proper athletic stance to balance and get you used to going down the hill in a safe, controlled setting. What do you need next? To be able to stop yourself. Here, your coach will teach you how to turn your skis into a wedge, also commonly referred to as a pizza, by rotating legs inward and pushing out on the heels. Once learned, you practice a gliding wedge down a small hill where you gradually come to a stop on the flat ground thanks to your wedge. Finally, you learn the necessary skill of getting up after falling, which is much easier than it looks, but once learned, a piece of cake.\n",
        "Summary: Skiing is a great way to enjoy the outdoors and get some exercise. It can be a little daunting at first, but with a little practice, you\\'ll be skiing like a pro in no time.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: Yellowstone National Park is an American national park located in the western United States, largely in the northwest corner of Wyoming and extending into Montana and Idaho. It was established by the 42nd U.S. Congress with the Yellowstone National Park Protection Act and signed into law by President Ulysses S. Grant on March 1, 1872. Yellowstone was the first national park in the U.S. and is also widely held to be the first national park in the world.The park is known for its wildlife and its many geothermal features, especially the Old Faithful geyser, one of its most popular. While it represents many types of biomes, the subalpine forest is the most abundant. It is part of the South Central Rockies forests ecoregion.\n",
        "Summary: Yellowstone National Park is the first national park in the United States and the world. It is located in the western United States, largely in the northwest corner of Wyoming and extending into Montana and Idaho. The park is known for its wildlife and its many geothermal features, especially the Old Faithful geyser.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: While audio-visual speech models can yield superior performance and robustness\n",
        "compared to audio-only models, their development and adoption are hindered\n",
        "by the lack of labeled and unlabeled audio-visual data and the cost to deploy\n",
        "one model per modality. In this paper, we present u-HuBERT, a self-supervised\n",
        "pre-training framework that can leverage both multimodal and unimodal speech\n",
        "with a unified masked cluster prediction objective. By utilizing modality dropout\n",
        "during pre-training, we demonstrate that a single fine-tuned model can achieve\n",
        "performance on par or better than the state-of-the-art modality-specific models.\n",
        "Moreover, our model fine-tuned only on audio can perform well with audio-visual\n",
        "and visual speech input, achieving zero-shot modality generalization for multiple\n",
        "speech processing tasks. In particular, our single model yields 1.2%/1.4%/27.2%\n",
        "speech recognition word error rate on LRS3 with audio-visual/audio/visual input.\n",
        "Summary: u-HuBERT is a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. By utilizing modality dropout during pre-training, u-HuBERT can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, u-HuBERT can be fine-tuned only on audio and still perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: Generative models of code, pretrained on large corpora of programs, have shown great success in translating natural language to code (Chen et al., 2021; Austin et al., 2021; Li et al., 2022, inter alia). While these models do not explicitly incorporate program semantics (i.e., execution results) during training, they are able to generate correct solutions for many problems. However, choosing a single correct program from a generated set for each problem remains challenging.\n",
        "\n",
        "In this work, we introduce execution result– based minimum Bayes risk decoding (MBREXEC) for program selection and show that it improves the few-shot performance of pretrained code models on natural-language-to-code tasks. We select output programs from a generated candidate set by marginalizing over program implementations that share the same semantics. Because exact equivalence is intractable, we execute each program on a small number of test inputs to approximate semantic equivalence. Across datasets, execution or simulated execution significantly outperforms the methods that do not involve program semantics. We find that MBR-EXEC consistently improves over all execution-unaware selection methods, suggesting it as an effective approach for natural language to code translation.\n",
        "Summary:\n",
        "\"\"\",\n",
        "    **parameters\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58Y1IJRpDqUS",
        "outputId": "ecb8983f-ad39-488e-bde1-42dc093e244e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: Generative models of code have shown great success in translating natural language to code. However, choosing a single correct program from a generated set for each problem remains challenging. This paper introduces execution result-based minimum Bayes risk decoding (MBREXEC) for program selection and shows that it improves the few-shot performance of pretrained code models on natural-language-to-code tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TESTING READING DATA FROM GOOGLE SHEETS"
      ],
      "metadata": {
        "id": "y2XAO7zjFkaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "worksheet = gc.open('ML - Research').sheet1\n"
      ],
      "metadata": {
        "id": "edlnnIN6FOqi"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "print(type(rows))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKgw6FJ7FuWs",
        "outputId": "9955ae92-5d5b-4eeb-f90f-7e1d280aa215"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7VDIsv3F0OA",
        "outputId": "b88a40e1-e27d-474f-9d50-5446b6c05432"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['prefix', 'arxiv_id_num', 'arxiv_id', 'title', 'description'], ['arxiv:', '2306.12929', 'arxiv:2306.12929', 'Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing', 'Transformer models have been shown to learn strong outliers in their activations, making them difficult to quantize. We propose two simple modifications to the attention mechanism that reduce outliers while maintaining performance, enabling full INT8 quantization of the activations.\\n'], ['', '2306.00739', 'arxiv:2306.00739', 'SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL', 'Large language models (LLMs) can generate code like SQL for databases. This paper proposes SQL-PaLM, an LLM-based Text-to-SQL model that achieves state-of-the-art results. It achieves 77.3% accuracy on the Spider test suite, outperforming previous models by a significant margin. It also demonstrates robustness and generalization capabilities in real-world scenarios. Extensive case studies highlight the intelligent capabilities of LLM-based Text-to-SQL.'], ['', '2302.13971', 'arxiv:2302.13971', 'LLaMA: Open and Efficient Foundation Language Models', 'LLaMA is a collection of foundation language models ranging from 7B to 65B parameters. They are trained on trillions of tokens using publicly available datasets, and outperform GPT-3 (175B) and other state-of-the-art models.'], ['', '2306.13085', 'arxiv:2306.13085', 'Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting\\r', ''], ['', '', 'arxiv:', '', ''], ['', '', 'arxiv:', '', ''], ['', '', 'arxiv:', '', ''], ['', '', 'arxiv:', '', ''], ['', '', 'arxiv:', '', ''], ['', '', 'arxiv:', '', '']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# Create the pandas DataFrame\n",
        "df = pd.DataFrame(rows, columns = ['prefix','arxiv_id_num',\t'arxiv_id','title',\t'description'])\n",
        "# print dataframe.\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX3OazoCGZ04",
        "outputId": "50bd0830-98f1-4cb5-fd66-4199bb4970ba"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    prefix  arxiv_id_num          arxiv_id  \\\n",
            "0   prefix  arxiv_id_num          arxiv_id   \n",
            "1   arxiv:    2306.12929  arxiv:2306.12929   \n",
            "2             2306.00739  arxiv:2306.00739   \n",
            "3             2302.13971  arxiv:2302.13971   \n",
            "4             2306.13085  arxiv:2306.13085   \n",
            "5                                   arxiv:   \n",
            "6                                   arxiv:   \n",
            "7                                   arxiv:   \n",
            "8                                   arxiv:   \n",
            "9                                   arxiv:   \n",
            "10                                  arxiv:   \n",
            "\n",
            "                                                title  \\\n",
            "0                                               title   \n",
            "1   Quantizable Transformers: Removing Outliers by...   \n",
            "2   SQL-PaLM: Improved Large Language Model Adapta...   \n",
            "3   LLaMA: Open and Efficient Foundation Language ...   \n",
            "4   Harnessing Mixed Offline Reinforcement Learnin...   \n",
            "5                                                       \n",
            "6                                                       \n",
            "7                                                       \n",
            "8                                                       \n",
            "9                                                       \n",
            "10                                                      \n",
            "\n",
            "                                          description  \n",
            "0                                         description  \n",
            "1   Transformer models have been shown to learn st...  \n",
            "2   Large language models (LLMs) can generate code...  \n",
            "3   LLaMA is a collection of foundation language m...  \n",
            "4                                                      \n",
            "5                                                      \n",
            "6                                                      \n",
            "7                                                      \n",
            "8                                                      \n",
            "9                                                      \n",
            "10                                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TESTING ARXIV SCRAPING FOR ABSTRACT"
      ],
      "metadata": {
        "id": "Ap42fK8bIesh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k4zIQTrHEr2",
        "outputId": "c8cbb14e-239f-4a85-f630-3cf3c93010b6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['arxiv_id'][4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x4FI1UHyJL5P",
        "outputId": "8e4d5af3-cfd0-41ae-ad24-4910ff704ecd"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'arxiv:2306.13085'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def get_abstract(arxiv_id):\n",
        "  url = f'http://arxiv.org/abs/{arxiv_id}'\n",
        "  uh = urllib.request.urlopen(url)\n",
        "  data = uh.read()\n",
        "  soup = BeautifulSoup(data, 'html.parser')\n",
        "  abstract = soup.find('blockquote', {'class': 'abstract mathjax'})\n",
        "  if abstract:\n",
        "    return abstract.text.strip()\n",
        "  else:\n",
        "    return ''\n",
        "\n",
        "arxiv_id = df['arxiv_id'][4] # Replace with your desired arXiv ID\n",
        "abstract = get_abstract(arxiv_id)\n",
        "print(abstract[11:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgZ8DPmsItCb",
        "outputId": "4757cede-b0e6-4e17-877f-d647803a1be7"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most offline reinforcement learning (RL) algorithms return a target policy\n",
            "maximizing a trade-off between (1) the expected performance gain over the\n",
            "behavior policy that collected the dataset, and (2) the risk stemming from the\n",
            "out-of-distribution-ness of the induced state-action occupancy. It follows that\n",
            "the performance of the target policy is strongly related to the performance of\n",
            "the behavior policy and, thus, the trajectory return distribution of the\n",
            "dataset. We show that in mixed datasets consisting of mostly low-return\n",
            "trajectories and minor high-return trajectories, state-of-the-art offline RL\n",
            "algorithms are overly restrained by low-return trajectories and fail to exploit\n",
            "high-performing trajectories to the fullest. To overcome this issue, we show\n",
            "that, in deterministic MDPs with stochastic initial states, the dataset\n",
            "sampling can be re-weighted to induce an artificial dataset whose behavior\n",
            "policy has a higher return. This re-weighted sampling strategy may be combined\n",
            "with any offline RL algorithm. We further analyze that the opportunity for\n",
            "performance improvement over the behavior policy correlates with the\n",
            "positive-sided variance of the returns of the trajectories in the dataset. We\n",
            "empirically show that while CQL, IQL, and TD3+BC achieve only a part of this\n",
            "potential policy improvement, these same algorithms combined with our\n",
            "reweighted sampling strategy fully exploit the dataset. Furthermore, we\n",
            "empirically demonstrate that, despite its theoretical limitation, the approach\n",
            "may still be efficient in stochastic environments. The code is available at\n",
            "this https URL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_input_prompt_with_example = \"\"\"Provide a summary with about two sentences for the following article: Beyond our own products, we think it\\'s important to make it easy, safe and scalable for others to benefit from these advances by building on top of our best models. Next month, we\\'ll start onboarding individual developers, creators and enterprises so they can try our Generative Language API, initially powered by LaMDA with a range of models to follow. Over time, we intend to create a suite of tools and APIs that will make it easy for others to build more innovative applications with AI. Having the necessary compute power to build reliable and trustworthy AI systems is also crucial to startups, and we are excited to help scale these efforts through our Google Cloud partnerships with Cohere, C3.ai and Anthropic, which was just announced last week. Stay tuned for more developer details soon.\n",
        "Summary: Google is making its AI technology more accessible to developers, creators, and enterprises. Next month, Google will start onboarding developers to try its Generative Language API, which will initially be powered by LaMDA. Over time, Google intends to create a suite of tools and APIs that will make it easy for others to build more innovative applications with AI. Google is also excited to help scale these efforts through its Google Cloud partnerships with Cohere, C3.ai, and Anthropic.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: The benefits of electricPromptData kitchens go beyond climate impact, starting with speed. The first time I ever cooked on induction (electric) equipment, the biggest surprise was just how incredibly fast it is. In fact, induction boils water twice as fast as traditional gas equipment and is far more efficient — because unlike a flame, electric heat has nowhere to escape. At Bay View, our training programs help Google chefs appreciate and adjust to the new pace of induction. The speed truly opens up whole new ways of cooking.\n",
        "Summary: Electric kitchens are faster, more efficient, and better for the environment than gas kitchens. Induction cooking is particularly fast, boiling water twice as fast as traditional gas equipment. This speed opens up whole new ways of cooking. Google chefs are trained to appreciate and adjust to the new pace of induction cooking at Bay View.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: We\\'re also using AI to forecast floods, another extreme weather pattern exacerbated by climate change. We\\'ve already helped communities to predict when floods will hit and how deep the waters will get — in 2021, we sent 115 million flood alert notifications to 23 million people over Google Search and Maps, helping save countless lives. Today, we\\'re sharing that we\\'re now expanding our coverage to more countries in South America (Brazil and Colombia), Sub-Saharan Africa (Burkina Faso, Cameroon, Chad, Democratic Republic of Congo, Ivory Coast, Ghana, Guinea, Malawi, Nigeria, Sierra Leone, Angola, South Sudan, Namibia, Liberia, and South Africa), and South Asia (Sri Lanka). We\\'ve used an AI technique called transfer learning to make it work in areas where there\\'s less data available. We\\'re also announcing the global launch of Google FloodHub, a new platform that displays when and where floods may occur. We\\'ll also be bringing this information to Google Search and Maps in the future to help more people to reach safety in flooding situations.\n",
        "Summary: Google is using AI to forecast floods in South America, Sub-Saharan Africa, South Asia, and other parts of the world. The AI technique of transfer learning is being used to make it work in areas where there\\'s less data available. Google FloodHub, a new platform that displays when and where floods may occur, has also been launched globally. This information will also be brought to Google Search and Maps in the future to help more people reach safety in flooding situations.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: In order to learn skiing, you must first be educated on the proper use of the equipment. This includes learning how to properly fit your boot on your foot, understand the different functions of the ski, and bring gloves, goggles etc. Your instructor starts you with one-footed ski drills. Stepping side-to-side, forward-and-backward, making snow angels while keeping your ski flat to the ground, and gliding with the foot not attached to a ski up for several seconds. Then you can put on both skis and get used to doing them with two skis on at once. Next, before going down the hill, you must first learn how to walk on the flat ground and up small hills through two methods, known as side stepping and herringbone. Now it\\'s time to get skiing! For your first attempted run, you will use the skills you just learned on walking up the hill, to go down a small five foot vertical straight run, in which you will naturally stop on the flat ground. This makes you learn the proper athletic stance to balance and get you used to going down the hill in a safe, controlled setting. What do you need next? To be able to stop yourself. Here, your coach will teach you how to turn your skis into a wedge, also commonly referred to as a pizza, by rotating legs inward and pushing out on the heels. Once learned, you practice a gliding wedge down a small hill where you gradually come to a stop on the flat ground thanks to your wedge. Finally, you learn the necessary skill of getting up after falling, which is much easier than it looks, but once learned, a piece of cake.\n",
        "Summary: Skiing is a great way to enjoy the outdoors and get some exercise. It can be a little daunting at first, but with a little practice, you\\'ll be skiing like a pro in no time.\n",
        "\n",
        "Provide a summary with about two sentences for the following article: Yellowstone National Park is an American national park located in the western United States, largely in the northwest corner of Wyoming and extending into Montana and Idaho. It was established by the 42nd U.S. Congress with the Yellowstone National Park Protection Act and signed into law by President Ulysses S. Grant on March 1, 1872. Yellowstone was the first national park in the U.S. and is also widely held to be the first national park in the world.The park is known for its wildlife and its many geothermal features, especially the Old Faithful geyser, one of its most popular. While it represents many types of biomes, the subalpine forest is the most abundant. It is part of the South Central Rockies forests ecoregion.\n",
        "Summary: Yellowstone National Park is the first national park in the United States and the world. It is located in the western United States, largely in the northwest corner of Wyoming and extending into Montana and Idaho. The park is known for its wildlife and its many geothermal features, especially the Old Faithful geyser.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Tp1Rr5s8JX4S"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"Provide a summary with about two sentences for the following article and keep numerical data as it is and word count around 60: \""
      ],
      "metadata": {
        "id": "ptf7CGVdLjc4"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_input_with_example_prompt_test_abstract = model_input_prompt_with_example + PROMPT + abstract[11:] + \"Summary: \""
      ],
      "metadata": {
        "id": "OtXjjqgCKC2r"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.predict(model_input_with_example_prompt_test_abstract,**parameters)\n",
        "print(f\"Response from Model: {response.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da3Cm1opKX-C",
        "outputId": "5ceec8c8-4a10-4879-ce42-7d846c26bc93"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: Offline reinforcement learning algorithms are often constrained by the performance of the behavior policy that collected the dataset. This can lead to sub-optimal performance when the dataset contains mostly low-return trajectories. We propose a re-weighted sampling strategy that can be combined with any offline RL algorithm to improve performance. Our approach is theoretically grounded and empirically demonstrates that it can fully exploit the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UPDATE GOOGLE SHEETS WITH NEW ABSTRACT\n"
      ],
      "metadata": {
        "id": "9_0xJ9EpQezz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "worksheet.update('E5',response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFB-yVbEKlKh",
        "outputId": "5b43a104-aa69-419a-d25f-0b2039958354"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'spreadsheetId': '1hFf8s2YOFWo2G3LJfWQVICD-suTpf46SkzSekNfFlW8',\n",
              " 'updatedRange': 'ID_Name_Description_Domain!E5',\n",
              " 'updatedRows': 1,\n",
              " 'updatedColumns': 1,\n",
              " 'updatedCells': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NEXT STEPS,\n",
        "- read worksheet\n",
        "- create new abstract\n",
        "- update abstract all in one definition😁"
      ],
      "metadata": {
        "id": "6dN4xFMhQxBk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hompdgB2Q3PA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}